# -*- coding: utf-8 -*-
"""main_rnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ljpjCLW7rFNc7LgnfCFEcl7ZPgDyutgl
"""

###########################
# Mount Google Drive      #
###########################

results_dir = 'results'
model_dir = 'saved_models'
ptb_dir = 'ptb'

import torch
from torch import nn
import torch.nn.utils.prune as prune
from torch.nn.utils import clip_grad_norm_
import numpy as np
from torch import optim
import copy
import time
from collections import Counter
from torch.autograd import Variable
import pickle

from functional_models.architectures import *
from functional_models.eval_train import *
from functional_models.pruning_funcs import *


###########################
# Datasets                #
###########################

def load_text(path_dir):
    with open(path_dir + '/ptb.train.txt', encoding='utf-8') as f:
        train_text = f.readlines()
    with open(path_dir + '/ptb.valid.txt', encoding='utf-8') as f:
        valid_text = f.readlines()
    with open(path_dir + '/ptb.test.txt', encoding='utf-8') as f:
        test_text = f.readlines()

    return train_text, valid_text, test_text


###########################
# Text Preprocess         #
###########################

flatten = lambda l: [item for sublist in l for item in sublist]


def text_preprocess(text, word2index=None):
    text = flatten([sentence.strip().split() + ['<eos>'] for sentence in text])

    if word2index == None:
        word_counts = Counter(text)
        vocab = sorted(word_counts, key=word_counts.get, reverse=True)
        word2index = {'<unk>': 0}
        for v in vocab:
            if word2index.get(v) is None:
                word2index[v] = len(word2index)

    tokenized_text = list(
        map(lambda word: word2index[word] if word2index.get(word) is not None else word2index["<unk>"], text))

    return torch.cuda.LongTensor(tokenized_text), word2index


###########################
# Batch Creation.         #
###########################
def create_batch(data, batch_size, seq_len):
    n_batches = data.size(0) // batch_size
    data = data.narrow(0, 0, n_batches * batch_size)
    data = data.view(batch_size, -1).contiguous()
    data = data.cuda()

    for i in range(0, data.size(1) - seq_len, seq_len):
        inputs = Variable(data[:, i: i + seq_len])
        targets = Variable(data[:, (i + 1): (i + 1) + seq_len].contiguous())
        yield (inputs, targets)


###########################
# Network Architecture    #
###########################

class ptb_rnn(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, keep_prob=0.5, cell_type='lstm',
                 int_range=0.05):
        super().__init__()
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        self.cell_type = cell_type

        # Embedding layer (Encoder)
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # LSTM layers
        if self.cell_type == 'lstm':
            self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=1 - keep_prob, batch_first=True)
        # GRU layers
        elif self.cell_type == 'gru':
            self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=1 - keep_prob, batch_first=True)

        # Dropout layer
        self.dropout = nn.Dropout(1 - keep_prob)

        # Linear layers (Decoder)
        self.fc = nn.Linear(hidden_dim, vocab_size)

        # Same weights for Decoder and Encoder
        self.fc.weight = self.embedding.weight

        self.rand_initialize_weights()

    def rand_initialize_weights(self):
        for layer in self.modules():
            if isinstance(layer, nn.Linear):
                if prune.is_pruned(self):
                    torch.nn.init.xavier_normal_(layer.weight_orig)
                else:
                    torch.nn.init.xavier_normal_(layer.weight)
                if layer.bias is not None:
                    torch.nn.init.constant_(layer.bias, 0)
            elif isinstance(layer, nn.LSTM) or isinstance(layer, nn.GRU):
                for name, param in layer.named_parameters():
                    if 'weight_ih' in name:
                        torch.nn.init.xavier_normal_(param.data)
                    elif 'weight_hh' in name:
                        torch.nn.init.xavier_normal_(param.data)
                    elif 'bias' in name:
                        param.data.fill_(0)

    def forward(self, x, hidden):
        # Embedding layer
        x = self.embedding(x)

        # Dropout
        x = self.dropout(x)

        # LSTM / GRU
        if self.cell_type == 'lstm':
            x, hidden = self.lstm(x, hidden)
        elif self.cell_type == 'gru':
            x, hidden = self.gru(x, hidden)

        x = x.contiguous().view(-1, self.hidden_dim)

        # Dropout
        x = self.dropout(x)

        # Fully-connected layer
        output = self.fc(x)

        return output, hidden

    def init_hidden(self, batch_size):
        weight = next(self.parameters()).data
        if self.cell_type == 'lstm':
            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),
                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())
        elif self.cell_type == 'gru':
            return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()
        return hidden

    def detach_hidden(self, hidden):
        if self.cell_type == 'lstm':
            return tuple([each.detach() for each in hidden])
        elif self.cell_type == 'gru':
            return hidden.detach()


###########################
# Evaluation              #
###########################
def EvaluateModel_RNN(model, criterion, data, batch_size, seq_len):
    losses = []
    h = model.init_hidden(batch_size)
    for input, target in create_batch(data, batch_size, seq_len):
        input, target = input.cuda(), target.cuda()
        h = model.detach_hidden(h)
        output, h = model(input, h)
        loss = criterion(output, target.view(batch_size * seq_len))
        losses.append(loss.item())

    mean_loss = np.mean(losses)
    mean_pp = np.exp(mean_loss)

    return mean_loss, mean_pp


###########################
# Training                #
###########################
def TrainAndEvaluateModel_RNN(model, model_name, results_dir=None, model_dir=None, train_text=None, valid_text=None,
                              test_text=None, epochs=20, init_lr=0.001, batch_size=10, seq_len=50, clip=5):
    model_res_path = results_dir + '/' + model_name + '.pkl'
    model_name = model_dir + '/' + model_name + '.pt'
    print(model_res_path)

    model.cuda()

    # create data
    train_data, vocab = text_preprocess(train_text)
    valid_data, _ = text_preprocess(valid_text, vocab)
    test_data, _ = text_preprocess(test_text, vocab)

    # Define the loss and optimizer
    criterion = nn.CrossEntropyLoss().cuda()
    adapt_lr = init_lr

    valid_loss_min = np.Inf

    # init results data structure
    results = {'accs_losses': {'train_loss': [], 'train_acc': [],
                               'test_loss': [], 'test_acc': []},
               'min_valid_loss_data': {'min_valid_loss_val': np.Inf, 'min_valid_loss_iter': 0}}

    for epoch in range(epochs):
        t0 = time.time()

        # Train the model
        model.train()

        # Initialize
        h = model.init_hidden(batch_size)

        for input, target in create_batch(train_data, batch_size, seq_len):
            input, target = input.cuda(), target.cuda()
            h = model.detach_hidden(h)
            model.zero_grad()
            output, h = model(input, h)
            loss = criterion(output, target.view(batch_size * seq_len))
            loss.backward()
            clip_grad_norm_(model.parameters(), clip)
            for p in model.parameters():
                p.data.add_(-adapt_lr, p.grad.data)

        # Evaluate the model
        model.eval()

        with torch.no_grad():
            train_loss, train_pp = EvaluateModel_RNN(model, criterion, train_data, batch_size // 2, seq_len)
            valid_loss, valid_pp = EvaluateModel_RNN(model, criterion, valid_data, batch_size // 2, seq_len)
            test_loss, test_pp = EvaluateModel_RNN(model, criterion, test_data, batch_size // 2, seq_len)

        print(
            'Epoch: {} ({:.2f} seconds) Learning rate: {:.6f} \n Train Loss: {:.6f} Train Perplexity: {:.6f} Validation Loss: {:.6f} Validation Perplexity {:4.2f} Test Loss: {:.6f} Test Perplexity: {:.6f}'
            .format(epoch + 1, time.time() - t0, adapt_lr, train_loss, train_pp, valid_loss, valid_pp, test_loss,
                    test_pp))

        # Save model if validation loss has decreased
        if valid_loss < valid_loss_min:
            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))
            torch.save(model.state_dict(), model_name)
            valid_loss_min = valid_loss
            # and also save test pp and loss
            results['min_valid_loss_data']['min_valid_loss_val'] = test_loss
            results['min_valid_loss_data']['min_valid_loss_iter'] = epoch + 1
        elif valid_loss > valid_loss_last:  # Adjust learning rate if validation loss has increased
            print('Validation loss increased ({:.6f} --> {:.6f}). Adjust learning rate ...'.format(valid_loss_last,
                                                                                                   valid_loss))
            adapt_lr = adapt_lr / 2

        valid_loss_last = valid_loss

        results['accs_losses']['train_loss'].append(train_loss)
        results['accs_losses']['train_acc'].append(train_pp)
        results['accs_losses']['test_loss'].append(test_loss)
        results['accs_losses']['test_acc'].append(test_pp)

    print('Saving results...')
    with open(model_res_path, 'wb') as handle:
        pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)
    print('Saved')

##############################
# Parameters.                #
##############################

vocab_size = 10000
hidden_dim = 200
embedding_dim = hidden_dim
n_layers = 2
int_range = 0.1
init_lr = 20
clip = 0.1
epochs = 40
seed_used = 300
batch_size = 20
seq_len = 35
keep_prob = 0.7

model_type = 'lstm' # lstm | gru
approach = 'random'
method = 'local'
prune_ratio = 0.2
prune_ratio_conv = 0.2

if prune_ratio_conv is not None:
    prune_ratio_conv = prune_ratio_conv / 100
else:
    prune_ratio_conv = None

prune_output_layer = 1
winning_ticket_reinit = 0
prune_init = 'rewind'
rounds = 8

find_matching_tickets = 0
stabilize_epochs = 0
use_lr_scheduler = 0

##############################
# Train and Test Loaders.    #
##############################

train_text, valid_text, test_text = load_text(ptb_dir)
torch.manual_seed(seed_used)

##################
# Init. Model    #
##################

model = ptb_rnn(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim, n_layers=n_layers, cell_type=model_type, int_range=int_range, keep_prob=keep_prob)

#########################################
# Identifying winning/matching tickets. #
#########################################

if find_matching_tickets:
    print(f'Stabilizing {model_type} model, with before {approach} pruning...')
    TrainAndEvaluateModel_RNN(model,
                    f'model-{model_type}_stabilize',
                    init_lr=init_lr, batch_size=batch_size, seq_len=seq_len, clip=clip, epochs=epochs, model_dir=model_dir, results_dir=results_dir, train_text=train_text, valid_text=valid_text, test_text=test_text)

p_conv = None

print(
    f'Prune {model_type} model, with {approach} pruning and {prune_init} init, using pruning rate of {100 * prune_ratio:.1f}%')

if approach == "iterative":

    # Step 1
    init_prune_model(model)
    init_weights = save_model_weights(model)

    sparsity_l = []
    for round in range(rounds):
        print('Prune Round: {}'.format(round + 1))
        t0 = time.time()
        sparsity = calc_model_sparsity(model) / 100
        sparsity_l.append(sparsity)
        print_sparsity(model)

        # Step 2
        TrainAndEvaluateModel_RNN(model,
                    f'model-{model_type}_batchsz-{batch_size}_dp-{1-keep_prob:.1f}_approach-{approach}_method-{method}_init-{prune_init}_remainweights-{100 * (1 - sparsity):.1f}',
                    init_lr=init_lr, batch_size=batch_size, seq_len=seq_len, clip=clip, epochs=epochs, model_dir=model_dir, results_dir=results_dir, train_text=train_text, valid_text=valid_text, test_text=test_text)

        model_name = model_dir + '/' + f'model-{model_type}_batchsz-{batch_size}_dp-{1-keep_prob:.1f}_approach-{approach}_method-{method}_init-{prune_init}_remainweights-{100 * (1 - sparsity):.1f}' + '.pt'
        model.load_state_dict(torch.load(model_name))

        if round < (rounds - 1):
            # Step 3
            p = pow(prune_ratio, (1 / (round + 1)))
            if prune_ratio_conv is not None:
                p_conv = pow(prune_ratio_conv, (1 / (round + 1)))
            print(f'pruning rate: {100 * p:.1f}%')
            prune_model(model=model, prune_ratio=p, prune_ratio_conv=p_conv, prune_method=method,
                        prune_output_layer=prune_output_layer)

            # Step 4
            rewind_model_weights(model, init_weights)

    print(f'Model pruning, took {time.time() - t0: .2f} seconds')

elif approach == "oneshot":
    t0 = time.time()
    # Step 1
    init_prune_model(model)
    init_weights = save_model_weights(model)
    sparsity = calc_model_sparsity(model) / 100
    print_sparsity(model)

    for round in range(rounds):
        if round is not 0:
            # Step 2
            for i in range(round):
                p = pow(prune_ratio, (1 / (i + 1)))
                if prune_ratio_conv is not None:
                    p_conv = pow(prune_ratio_conv, (1 / (i + 1)))
                prune_model(model=model, prune_ratio=p, prune_ratio_conv=p_conv, prune_method=method,
                            prune_output_layer=prune_output_layer)

            sparsity = calc_model_sparsity(model) / 100
            print_sparsity(model)

            # Step 3
            rewind_model_weights(model, init_weights)

        # Step 4
        TrainAndEvaluateModel_RNN(model,
                    f'model-{model_type}_batchsz-{batch_size}_dp-{1-keep_prob:.1f}_approach-{approach}_method-{method}_init-{prune_init}_remainweights-{100 * (1 - sparsity):.1f}',
                    init_lr=init_lr, batch_size=batch_size, seq_len=seq_len, clip=clip, epochs=epochs, model_dir=model_dir, results_dir=results_dir, train_text=train_text, valid_text=valid_text, test_text=test_text)
        
        # Step 5
        init_prune_model(model)
        model_name = model_dir + '/' + f'model-{model_type}_batchsz-{batch_size}_dp-{1-keep_prob:.1f}_approach-{approach}_method-{method}_init-{prune_init}_remainweights-100.0' + '.pt'
        model.load_state_dict(torch.load(model_name))

    print(f'Model pruning, took {time.time() - t0: .2f} seconds')

elif approach == "random":
    t0 = time.time()
    # Step 1
    init_prune_model(model)

    for round in range(rounds):
        # Step 2
        for i in range(round):
            p = pow(prune_ratio, (1 / (i + 1)))
            if prune_ratio_conv is not None:
                p_conv = pow(prune_ratio_conv, (1 / (i + 1)))
            prune_model(model=model, prune_ratio=p, prune_ratio_conv=p_conv, prune_method=method,
                        prune_output_layer=prune_output_layer)

        sparsity = calc_model_sparsity(model) / 100
        print_sparsity(model)

        # Step 3
        TrainAndEvaluateModel_RNN(model,
                    f'model-{model_type}_batchsz-{batch_size}_dp-{1-keep_prob:.1f}_approach-{approach}_method-{method}_init-{prune_init}_remainweights-{100 * (1 - sparsity):.1f}',
                    init_lr=init_lr, batch_size=batch_size, seq_len=seq_len, clip=clip, epochs=epochs, model_dir=model_dir, results_dir=results_dir, train_text=train_text, valid_text=valid_text, test_text=test_text)

        # Step 4
        init_prune_model(model)
        model.rand_initialize_weights()

    print(f'Model pruning, took {time.time() - t0: .2f} seconds')